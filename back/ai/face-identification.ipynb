{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Identification\n",
    "\n",
    "1. MTCNN으로 이미지 내 얼굴을 인식하고 얼굴 부분만 오려낸다.\n",
    "\n",
    "2. 얼굴 이미지를  \"FaceNet: A Unified Embedding for Face Recognition and Clustering\"을 통해 특징 벡터로 표현한다.\n",
    "\n",
    "3. 이 특징 벡터로 SVM 분류기를 학습하여 Face Identification 모델을 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 이 코드가 적힌 파일 위치에 facenet_pytorch이라는 폴더가 있고, 이를 모듈처럼 불러와 사용하는데,\n",
    "\n",
    "이것도 `__init__.py`라는 파일이 있어야 이렇게 모듈처럼 불러올 수 있고, 동일 폴더 내에 있어야 하는 등 여러 조건이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "# 구글이 공개한 facenet이라는 AI 모델이 있는데, tensorflow로 작성된 것을 pytorch 형태로 공개하고 있다.\n",
    "# https://github.com/timesler/facenet-pytorch\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 경고 메시지 무시하기\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "workers = 0 if os.name == 'nt' else 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine if an nvidia GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 봤었던 MNIST의 경우, 데이터가 이미지인데 excel에 숫자로 적혀 있어서 pandas로 불러와서 이를 torch로 바꿔주었으나,\n",
    "\n",
    "`featuresTrain = torch.from_numpy(features_train)`\n",
    "\n",
    "`targetsTrain = torch.from_numpy(target_train).type(torch.LongTensor)`\n",
    "\n",
    "`train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)`\n",
    "\n",
    "여기서는 이미지 파일이기 때문에 torchvision.datasets이라는 내장된 모듈을 사용해 간편하게 불러온다.\n",
    "\n",
    "***그리고 이미지를 align 해야 하는 과정이 필요하다.***\n",
    "\n",
    "`dataset = datasets.ImageFolder(data_path)`\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "***추가로, [Pytorch를 활용한 데이터 불러오는 과정에 대한 튜토리얼](https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch를 사용하여 이미지를 불러올 때\n",
    "\n",
    "1. 보통 `torchvision.datasets`로 데이터 객체를 선언하고, (`torchvision.datasets`은 `torch.utils.data.Dataset`의 서브 클래스이다)\n",
    "2. `torch.utils.data.DataLoader`로 데이터 로드를 위한 객체를 선언하고,\n",
    "3. 그 객체로 데이터를 로드한다.\n",
    "\n",
    "데이터를 불러오는 과정에서 이러한 내장된 객체를 사용하는 이유는 여러 기능을 포함하기 때문이다.\n",
    "\n",
    "`torchvision.datasets`의 경우,\n",
    "\n",
    "- IMG_EXTENSIONS('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')에 대해서 처리 가능하고,\n",
    "- 데이터를 PIL(Python Imaging Library) 객체로 바뀌어 python 내에서 통일적으로 효율적으로 처리된다.\n",
    "- 또한 이미지를 쉽게 변형시킬 수 있는 transform 기능을 포함하고 있다.\n",
    "- 마지막으로 datasets 내에는 유명한 데이터셋을 쉽게 불러오거나 다운로드 받을 수 있고 있고, (`torchvision.datasets.MNIST`)\n",
    "- 폴더 경로만 주어지면 자동으로 index를 포함하여 데이터셋을 만들 수 있는 객체도 포함한다. (`torchvision.datasets.ImageFolder`)\n",
    "\n",
    "`torch.utils.data.DataLoader`의 경우 아래의 5가지 대표적인 기능을 하며,\n",
    "\n",
    "- map-style and iterable-style datasets, (map은 각 데이터에 대해서 같은 처리를 동시에 하는 방식이고, iterable을 순회해가며 처리)\n",
    "- customizing data loading order,\n",
    "- automatic batching,\n",
    "- single- and multi-process data loading,\n",
    "- automatic memory pinning.\n",
    "\n",
    "`torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None)`\n",
    "\n",
    "위를 보면, 다양한 parameter를 볼 수 있는데, parameter 변경으로 데이터를 로드할 때 여러 처리를 할 수 있다.\n",
    "\n",
    "설명을 조금 하자면, Dataloader를 통해 batch로 혹은 non-batch로 데이터를 불러올 수 있는데, batch란 ***collating individual fetched data samples into batches***, 즉 개별 데이터를 batch 묶음 형태로 수집하는 것이다. Dataloader는 기본값으로 자동 batching을 지원한다.\n",
    "\n",
    "개별 데이터의 index들을 사용하여 sample list로 묶이면, collate_fn 함수를 통해 batches로 수집(collate)된다.\n",
    "\n",
    "```\n",
    "for indices in batch_sampler:\n",
    "    yield collate_fn([dataset[i] for i in indices])\n",
    "```\n",
    "\n",
    "중요한 것은,\n",
    "The use of collate_fn is slightly different when automatic batching is enabled or disabled.\n",
    "\n",
    "***When automatic batching is disabled***, collate_fn is called with each individual data sample, and the output is yielded from the data loader iterator. In this case, the default collate_fn simply converts NumPy arrays in PyTorch tensors.\n",
    "\n",
    "***When automatic batching is enabled***, collate_fn is called with a list of data samples at each time. It is expected to collate the input samples into a batch for yielding from the data loader iterator. The rest of this section describes behavior of the default collate_fn in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a dataset and data loader**\n",
    "\n",
    "We add the idx_to_class attribute to the dataset to enable easy recoding of label indices to identity names later one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "dataset = datasets.ImageFolder(\"./data/train\")\n",
    "dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()}\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/train\\\\IU\\\\IU_ (1).JPG', 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  <PIL.Image.Image image mode=RGB size=490x489 at 0x21ECBF02788>\n",
      "y:  0\n"
     ]
    }
   ],
   "source": [
    "for x, y in loader:\n",
    "    print(\"x: \", x)\n",
    "    print(\"y: \", y)\n",
    "    break\n",
    "    \n",
    "#  collate_fn에 따른 결과값, 순서대로 x, x[0], x[0][0]\n",
    "# [(<PIL.Image.Image image mode=RGB size=490x489 at 0x1AD3C10C7F0>, 0)]\n",
    "# (<PIL.Image.Image image mode=RGB size=490x489 at 0x1AD3C58BDD8>, 0)\n",
    "# <PIL.Image.Image image mode=RGB size=490x489 at 0x1AD3C56AE48>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define MTCNN module\n",
    "\n",
    "See `help(MTCNN)` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detected with probability: 0.998145\n",
      "Face detected with probability: 0.999908\n",
      "Face detected with probability: 0.999996\n",
      "Face detected with probability: 0.998552\n",
      "Face detected with probability: 0.993009\n",
      "Face detected with probability: 0.999828\n",
      "Face detected with probability: 0.999999\n",
      "Face detected with probability: 0.999686\n",
      "Face detected with probability: 0.999639\n",
      "Face detected with probability: 0.999997\n",
      "Face detected with probability: 0.999999\n",
      "Face detected with probability: 0.999991\n",
      "Face detected with probability: 0.999993\n",
      "Face detected with probability: 0.999198\n",
      "Face detected with probability: 0.999426\n",
      "Face detected with probability: 0.999996\n",
      "Face detected with probability: 0.999999\n",
      "Face detected with probability: 0.999957\n",
      "Face detected with probability: 0.999995\n",
      "Face detected with probability: 0.999917\n",
      "Face detected with probability: 0.999325\n",
      "Face detected with probability: 0.999985\n",
      "Face detected with probability: 0.998141\n",
      "Face detected with probability: 0.999952\n",
      "Face detected with probability: 0.999968\n",
      "Face detected with probability: 0.999132\n",
      "Face detected with probability: 0.999968\n",
      "Face detected with probability: 0.999975\n",
      "Face detected with probability: 0.999495\n",
      "Face detected with probability: 0.999447\n",
      "Face detected with probability: 0.999995\n",
      "Face detected with probability: 0.999757\n",
      "Face detected with probability: 0.999955\n",
      "Face detected with probability: 0.997439\n",
      "Face detected with probability: 0.999678\n",
      "Face detected with probability: 0.999981\n",
      "Face detected with probability: 0.999668\n",
      "Face detected with probability: 0.999556\n",
      "Face detected with probability: 0.999735\n",
      "Face detected with probability: 0.999736\n",
      "Face detected with probability: 0.999994\n",
      "Face detected with probability: 0.993962\n",
      "Face detected with probability: 0.999187\n",
      "Face detected with probability: 0.999135\n",
      "Face detected with probability: 0.999309\n",
      "Face detected with probability: 0.999932\n",
      "Face detected with probability: 0.999169\n",
      "Face detected with probability: 0.999975\n",
      "Face detected with probability: 0.999306\n",
      "Face detected with probability: 0.999112\n"
     ]
    }
   ],
   "source": [
    "aligned = []\n",
    "indexes  = []\n",
    "names = []\n",
    "# x는 이미지(<PIL.Image.Image image mode=RGB size=490x489 at 0x1AD3C42EC50>)\n",
    "# y는 index\n",
    "for x, y in loader:\n",
    "    x_aligned, prob = mtcnn(x, return_prob=True)\n",
    "    if x_aligned is not None:\n",
    "        print('Face detected with probability: {:8f}'.format(prob))\n",
    "        aligned.append(x_aligned)\n",
    "        indexes.append(y)\n",
    "        names.append(dataset.idx_to_class[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(pretrained='vggface2', classify=True).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned = torch.stack(aligned).to(device)\n",
    "embeddings = resnet(aligned).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.4150, 0.7234, 0.3455,  ..., 2.2291, 2.9832, 3.0438])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='linear', max_iter=-1, probability=True, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', probability=True)\n",
    "clf.fit(embeddings.tolist(), indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(pretrained='vggface2', classify=True).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "dataset = datasets.ImageFolder(\"./data/test\")\n",
    "dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()}\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned = []\n",
    "indexes = []\n",
    "names = []\n",
    "for x, y in loader:\n",
    "    x_aligned, prob = mtcnn(x, return_prob=True)\n",
    "    if x_aligned is not None:\n",
    "        aligned.append(x_aligned)\n",
    "        indexes.append(y)\n",
    "        names.append(dataset.idx_to_class[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 160, 160])\n"
     ]
    }
   ],
   "source": [
    "print(type(aligned))\n",
    "print(type(aligned[0]))\n",
    "print((aligned[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned = torch.stack(aligned).to(device)\n",
    "embeddings = resnet(aligned).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([20, 3, 160, 160])\n"
     ]
    }
   ],
   "source": [
    "print(type(aligned))\n",
    "print(aligned.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8631])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = clf.predict_proba(embeddings.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36982908 0.03570551 0.06124283 0.06139958 0.09503324 0.07133033\n",
      " 0.04158086 0.04414225 0.10127885 0.11845746]\n",
      "[0.25093076 0.04572338 0.05509211 0.07343471 0.12484083 0.04618338\n",
      " 0.04943881 0.04079879 0.10369446 0.20986278]\n",
      "[0.03534779 0.50369624 0.03039686 0.09328613 0.04144831 0.04644801\n",
      " 0.10265525 0.06475546 0.0276678  0.05429817]\n",
      "[0.04517789 0.45934537 0.03736134 0.10340671 0.03867177 0.04670841\n",
      " 0.12736922 0.06647125 0.0381958  0.03729225]\n",
      "[0.04089995 0.13146285 0.41598478 0.05982932 0.08816559 0.06555806\n",
      " 0.05564203 0.03384545 0.06986731 0.03874467]\n",
      "[0.05297299 0.09824389 0.43164673 0.04939878 0.0907243  0.05401391\n",
      " 0.05665086 0.06316787 0.05458602 0.04859465]\n",
      "[0.04219221 0.06841136 0.02922624 0.47242545 0.03327145 0.07968511\n",
      " 0.09455651 0.04205825 0.04159402 0.09657941]\n",
      "[0.05290119 0.09549128 0.03395533 0.46642297 0.04394313 0.09319337\n",
      " 0.07582471 0.04008945 0.04964924 0.04852934]\n",
      "[0.06853329 0.05696401 0.05751299 0.03359082 0.49603691 0.06251467\n",
      " 0.02629202 0.05572951 0.02589343 0.11693234]\n",
      "[0.10119778 0.08737378 0.03946965 0.03714309 0.49794189 0.04585976\n",
      " 0.0261903  0.05879949 0.0455016  0.06052266]\n",
      "[0.07595795 0.04966908 0.0691071  0.05540095 0.12367945 0.43434403\n",
      " 0.02948121 0.03938046 0.0465488  0.07643096]\n",
      "[0.12235935 0.04689931 0.08738719 0.03663674 0.16112522 0.33964183\n",
      " 0.02781456 0.04815127 0.04435396 0.08563056]\n",
      "[0.04199653 0.10542937 0.06348753 0.11064782 0.02717819 0.03504068\n",
      " 0.46784136 0.0622323  0.04121254 0.04493368]\n",
      "[0.0365805  0.11714934 0.0379001  0.07445605 0.02135314 0.03474874\n",
      " 0.5124299  0.08474481 0.03137385 0.04926357]\n",
      "[0.05051537 0.11970232 0.04357776 0.04212129 0.12206445 0.05346555\n",
      " 0.04017775 0.37465578 0.05299331 0.10072642]\n",
      "[0.03887399 0.0904582  0.03075819 0.08809116 0.03125315 0.04371835\n",
      " 0.10767705 0.47091566 0.03316463 0.06508963]\n",
      "[0.14364123 0.06085534 0.04351236 0.03056164 0.19917779 0.0780975\n",
      " 0.03205428 0.04171582 0.30505076 0.06533328]\n",
      "[0.14810132 0.067084   0.05987555 0.05188717 0.09862797 0.03390075\n",
      " 0.03414682 0.05155822 0.40183196 0.05298624]\n",
      "[0.18186124 0.06989142 0.08274025 0.03476401 0.09112637 0.10744118\n",
      " 0.05745266 0.14723615 0.15474071 0.07274601]\n",
      "[0.21164936 0.06133398 0.12176639 0.05863421 0.07464097 0.1130197\n",
      " 0.04283255 0.11212266 0.06950347 0.13449671]\n"
     ]
    }
   ],
   "source": [
    "for r in result:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 결과는 확률 상 가능 높은 수치 값과 index를 의미하는데,\n",
    "\n",
    "마지막 데이터셋은 일부러 오답을 넣어두었다. 윤하 폴더에 전소미 사진을 넣어두었다.\n",
    "\n",
    "전소미는 그냥 한국인인데 외국인 느낌이 나서 섭외해봤다.\n",
    "\n",
    "개인적인 호감은 없다.\n",
    "\n",
    "데이터를 보면 알 수 있듯이, 가장 높은 확률이 30 ~ 50 사이로 큰 차이로 형성되는 것을 볼 수 있다.\n",
    "\n",
    "가장 높은 확률을 정답으로 택하면 모두 맞히지만, 학습되지 않은 얼굴인 전소미에 대해서 21프로의 결과를 얻은 것을 보았을 때, 25프로 아래는 unknown으로 처리해야 할 수준이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
